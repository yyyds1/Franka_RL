#!/usr/bin/env python3
# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""
æµ‹è¯•Go2 Directç¯å¢ƒæ˜¯å¦æ­£ç¡®è®¾ç½®
"""

import argparse

# ğŸ”‘ å…³é”®: å¿…é¡»å…ˆå¯åŠ¨Isaac Simï¼Œå†å¯¼å…¥å…¶ä»–æ¨¡å—
from isaaclab.app import AppLauncher

# æ·»åŠ å‘½ä»¤è¡Œå‚æ•°
parser = argparse.ArgumentParser(description="Test Go2 Direct environment")
parser.add_argument("--num_steps", type=int, default=100, help="Number of steps to run")
parser.add_argument("--num_envs", type=int, default=16, help="Number of environments")

# æ·»åŠ AppLauncherå‚æ•° (ä¼šè‡ªåŠ¨æ·»åŠ  --headless, --device ç­‰å‚æ•°)
AppLauncher.add_app_launcher_args(parser)
args = parser.parse_args()

# ğŸ”‘ å¯åŠ¨Isaac Simåº”ç”¨
app_launcher = AppLauncher(args)
simulation_app = app_launcher.app

# ç°åœ¨å¯ä»¥å®‰å…¨å¯¼å…¥å…¶ä»–æ¨¡å—
import torch
import gymnasium as gym

# æ³¨å†ŒGo2ç¯å¢ƒ
import Franka_RL.tasks.direct.go2_rl


def test_go2_environment(num_steps: int = 100, num_envs: int = 16):
    """æµ‹è¯•Go2ç¯å¢ƒ"""
    
    print("=" * 80)
    print("Testing Go2 Direct Environment")
    print("=" * 80)
    
    # å¯¼å…¥é…ç½®ç±»
    from Franka_RL.tasks.direct.go2_rl.go2_env_cfg import Go2EnvCfg
    
    # åˆ›å»ºç¯å¢ƒé…ç½®
    env_cfg = Go2EnvCfg()
    env_cfg.scene.num_envs = num_envs
    
    # åˆ›å»ºç¯å¢ƒ
    print(f"\nğŸ”„ Creating environment with {num_envs} parallel environments...")
    env = gym.make("Isaac-Go2-Direct-v0", cfg=env_cfg)
    
    print(f"\nâœ… Environment created successfully!")
    print(f"  - Num envs: {env.unwrapped.num_envs}")
    print(f"  - Observation space: {env.observation_space}")
    print(f"  - Action space: {env.action_space}")
    print(f"  - Device: {env.unwrapped.device}")
    
    # é‡ç½®ç¯å¢ƒ
    print(f"\nğŸ”„ Resetting environment...")
    obs, info = env.reset()
    
    print(f"\nâœ… Environment reset successfully!")
    print(f"  - Observation shape: {obs['policy'].shape}")
    print(f"  - Expected shape: ({num_envs}, 48)")
    
    # æ£€æŸ¥è§‚æµ‹å€¼èŒƒå›´
    obs_policy = obs['policy']
    print(f"\nğŸ“Š Observation statistics:")
    print(f"  - Min: {obs_policy.min():.4f}")
    print(f"  - Max: {obs_policy.max():.4f}")
    print(f"  - Mean: {obs_policy.mean():.4f}")
    print(f"  - Std: {obs_policy.std():.4f}")
    
    # è¿è¡Œéšæœºç­–ç•¥
    print(f"\nğŸƒ Running {num_steps} steps with random policy...")
    
    total_reward = 0.0
    episode_count = 0
    step_rewards = []
    
    for step in range(num_steps):
        # éšæœºåŠ¨ä½œ [-1, 1]
        action = 2.0 * torch.rand(
            (num_envs, env.unwrapped.cfg.action_space),
            device=env.unwrapped.device
        ) - 1.0
        
        # æ‰§è¡Œæ­¥éª¤
        obs, reward, terminated, truncated, info = env.step(action)
        
        mean_reward = reward.mean().item()
        total_reward += mean_reward
        step_rewards.append(mean_reward)
        
        # æ‰“å°è¿›åº¦
        if (step + 1) % 20 == 0:
            print(f"\n  ğŸ“ˆ Step {step + 1}/{num_steps}")
            print(f"     - Avg reward: {mean_reward:.4f}")
            print(f"     - Min reward: {reward.min():.4f}")
            print(f"     - Max reward: {reward.max():.4f}")
            
            # æ‰“å°è¯¦ç»†å¥–åŠ±åˆ†è§£
            if "log" in env.unwrapped.extras:
                log = env.unwrapped.extras["log"]
                print(f"     - Reward breakdown:")
                for key, value in sorted(log.items()):
                    if key.startswith("rewards/"):
                        reward_name = key.replace("rewards/", "")
                        print(f"       â€¢ {reward_name}: {value:.4f}")
        
        # æ£€æŸ¥é‡ç½®
        done = terminated | truncated
        if done.any():
            num_done = done.sum().item()
            episode_count += num_done
            if num_done > 0:
                print(f"     âš ï¸  {num_done} environments reset")
    
    # æœ€ç»ˆç»Ÿè®¡
    print(f"\n" + "=" * 80)
    print("ğŸ“Š Final Statistics")
    print("=" * 80)
    print(f"  - Total episodes finished: {episode_count}")
    print(f"  - Average reward per step: {total_reward / num_steps:.4f}")
    print(f"  - Reward std: {torch.tensor(step_rewards).std():.4f}")
    print(f"  - Best step reward: {max(step_rewards):.4f}")
    print(f"  - Worst step reward: {min(step_rewards):.4f}")
    
    # æµ‹è¯•ç¯å¢ƒçŠ¶æ€
    print(f"\nğŸ” Environment State Check:")
    robot = env.unwrapped.robot
    print(f"  - Base position: {robot.data.root_state_w[0, :3]}")
    print(f"  - Base velocity: {robot.data.root_lin_vel_w[0]}")
    print(f"  - Joint positions (first 3): {robot.data.joint_pos[0, :3]}")
    print(f"  - Joint velocities (first 3): {robot.data.joint_vel[0, :3]}")
    
    # å…³é—­ç¯å¢ƒ
    print(f"\nğŸ”„ Closing environment...")
    env.close()
    print("âœ… Environment closed successfully!")
    
    return True


def main():
    """Main function"""
    
    try:
        success = test_go2_environment(
            num_steps=args.num_steps,
            num_envs=args.num_envs
        )
        
        if success:
            print("\n" + "=" * 80)
            print("ğŸ‰ All tests passed!")
            print("=" * 80)
            print("\nğŸ’¡ Next steps:")
            print("  1. Train with PPO:")
            print("     python scripts/rsl_rl/train.py --task Isaac-Go2-Direct-v0")
            print("\n  2. Test with more environments:")
            print(f"     python {__file__} --num_envs 256 --num_steps 500")
            print("\n  3. Run in headless mode for faster testing:")
            print(f"     python {__file__} --headless --num_steps 1000")
            
    except Exception as e:
        print("\n" + "=" * 80)
        print(f"âŒ Test failed with error:")
        print(f"   {type(e).__name__}: {e}")
        print("=" * 80)
        import traceback
        traceback.print_exc()
        raise
    
    finally:
        # å…³é—­ä»¿çœŸåº”ç”¨
        simulation_app.close()


if __name__ == "__main__":
    main()